{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "635bc808",
   "metadata": {},
   "source": [
    "### Tensorflow and keras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1ddf1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aec3bf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbd1c379",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "298749c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import load_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "880ce6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './clothing-dataset-small/train/hat'\n",
    "name = '00d94e21-5891-492e-be0e-792e7338c077.jpg'\n",
    "fullname = f'{path}/{name}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6aa44fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = load_img(fullname, target_size=(299, 299))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85d28fdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(299, 299, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array(img)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06bc8497",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[161,  15,  15],\n",
       "        [161,  15,  15],\n",
       "        [161,  15,  15],\n",
       "        ...,\n",
       "        [113,   9,   6],\n",
       "        [112,   8,   5],\n",
       "        [110,   8,   4]],\n",
       "\n",
       "       [[161,  15,  15],\n",
       "        [161,  15,  15],\n",
       "        [161,  15,  15],\n",
       "        ...,\n",
       "        [118,  10,   8],\n",
       "        [115,  10,   7],\n",
       "        [113,   9,   6]],\n",
       "\n",
       "       [[161,  15,  15],\n",
       "        [161,  15,  15],\n",
       "        [161,  15,  15],\n",
       "        ...,\n",
       "        [126,  12,  12],\n",
       "        [124,  12,  11],\n",
       "        [121,  11,  10]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[101,   5,   7],\n",
       "        [101,   5,   7],\n",
       "        [101,   5,   7],\n",
       "        ...,\n",
       "        [124,  10,  10],\n",
       "        [126,  12,  12],\n",
       "        [128,  14,  14]],\n",
       "\n",
       "       [[101,   5,   7],\n",
       "        [101,   5,   7],\n",
       "        [101,   5,   7],\n",
       "        ...,\n",
       "        [125,  11,  11],\n",
       "        [126,  12,  12],\n",
       "        [128,  14,  14]],\n",
       "\n",
       "       [[101,   5,   7],\n",
       "        [101,   5,   7],\n",
       "        [101,   5,   7],\n",
       "        ...,\n",
       "        [125,  11,  11],\n",
       "        [126,  12,  12],\n",
       "        [127,  13,  13]]], shape=(299, 299, 3), dtype=uint8)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad9ab52",
   "metadata": {},
   "source": [
    "### Pre-trained convolutional neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213e7b29",
   "metadata": {},
   "source": [
    "https://www.image-net.org/\n",
    "\n",
    "https://keras.io/api/applications/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "084134af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.xception import Xception\n",
    "from tensorflow.keras.applications.xception import preprocess_input\n",
    "from tensorflow.keras.applications.xception import decode_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80492b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Xception(weights='imagenet', input_shape=(299, 299, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d56c5ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9083097a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = preprocess_input(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b2b2f81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.26274514, -0.88235295, -0.88235295],\n",
       "        [ 0.26274514, -0.88235295, -0.88235295],\n",
       "        [ 0.26274514, -0.88235295, -0.88235295],\n",
       "        ...,\n",
       "        [-0.11372548, -0.92941177, -0.9529412 ],\n",
       "        [-0.12156862, -0.9372549 , -0.9607843 ],\n",
       "        [-0.1372549 , -0.9372549 , -0.96862745]],\n",
       "\n",
       "       [[ 0.26274514, -0.88235295, -0.88235295],\n",
       "        [ 0.26274514, -0.88235295, -0.88235295],\n",
       "        [ 0.26274514, -0.88235295, -0.88235295],\n",
       "        ...,\n",
       "        [-0.0745098 , -0.92156863, -0.9372549 ],\n",
       "        [-0.09803921, -0.92156863, -0.94509804],\n",
       "        [-0.11372548, -0.92941177, -0.9529412 ]],\n",
       "\n",
       "       [[ 0.26274514, -0.88235295, -0.88235295],\n",
       "        [ 0.26274514, -0.88235295, -0.88235295],\n",
       "        [ 0.26274514, -0.88235295, -0.88235295],\n",
       "        ...,\n",
       "        [-0.01176471, -0.90588236, -0.90588236],\n",
       "        [-0.02745098, -0.90588236, -0.9137255 ],\n",
       "        [-0.05098039, -0.9137255 , -0.92156863]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-0.20784312, -0.9607843 , -0.94509804],\n",
       "        [-0.20784312, -0.9607843 , -0.94509804],\n",
       "        [-0.20784312, -0.9607843 , -0.94509804],\n",
       "        ...,\n",
       "        [-0.02745098, -0.92156863, -0.92156863],\n",
       "        [-0.01176471, -0.90588236, -0.90588236],\n",
       "        [ 0.00392163, -0.8901961 , -0.8901961 ]],\n",
       "\n",
       "       [[-0.20784312, -0.9607843 , -0.94509804],\n",
       "        [-0.20784312, -0.9607843 , -0.94509804],\n",
       "        [-0.20784312, -0.9607843 , -0.94509804],\n",
       "        ...,\n",
       "        [-0.01960784, -0.9137255 , -0.9137255 ],\n",
       "        [-0.01176471, -0.90588236, -0.90588236],\n",
       "        [ 0.00392163, -0.8901961 , -0.8901961 ]],\n",
       "\n",
       "       [[-0.20784312, -0.9607843 , -0.94509804],\n",
       "        [-0.20784312, -0.9607843 , -0.94509804],\n",
       "        [-0.20784312, -0.9607843 , -0.94509804],\n",
       "        ...,\n",
       "        [-0.01960784, -0.9137255 , -0.9137255 ],\n",
       "        [-0.01176471, -0.90588236, -0.90588236],\n",
       "        [-0.00392157, -0.8980392 , -0.8980392 ]]],\n",
       "      shape=(299, 299, 3), dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c861d2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "11a4abb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1000)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ed2d3fb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('n03424325', 'gasmask', np.float32(0.1535756)),\n",
       "  ('n02769748', 'backpack', np.float32(0.089838855)),\n",
       "  ('n03709823', 'mailbag', np.float32(0.064012595)),\n",
       "  ('n03595614', 'jersey', np.float32(0.04093369)),\n",
       "  ('n02834397', 'bib', np.float32(0.024432804))]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_predictions(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7cca64",
   "metadata": {},
   "source": [
    "### Transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "916615df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "468885f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3068 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "train_gen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "train_ds = train_gen.flow_from_directory('./clothing-dataset-small/train'\n",
    "                                         , target_size=(150, 150), \n",
    "                                         class_mode='sparse', \n",
    "                                         batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6c42ccd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dress': 0,\n",
       " 'hat': 1,\n",
       " 'longsleeve': 2,\n",
       " 'outwear': 3,\n",
       " 'pants': 4,\n",
       " 'shirt': 5,\n",
       " 'shoes': 6,\n",
       " 'shorts': 7,\n",
       " 'skirt': 8,\n",
       " 't-shirt': 9}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2029ef14",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = next(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "43d7c71d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 341 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "val_gen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "val_ds = val_gen.flow_from_directory('./clothing-dataset-small/validation'\n",
    "                                       , target_size=(150, 150), \n",
    "                                       class_mode='sparse',\n",
    "                                       shuffle=False, \n",
    "                                       batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "18d45d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#9:43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ade5a81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
